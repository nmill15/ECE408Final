{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import Audio\n",
    "import librosa, librosa.display\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from audiomentations import Compose, AddGaussianNoise, Gain, PitchShift, Shift\n",
    "import torch\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchinfo import summary\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchaudio\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mel_spectrogram(samples: np.array, sr: int = 44100, \n",
    "                        ) -> np.array:\n",
    "    \"\"\"\n",
    "    :param samples (np.array): samples array of the audio\n",
    "    :param sr (int): sample rate used for the samples\n",
    "    :return mel_spectrogram (np.array): np.array containing \n",
    "                                      melspectrogram features in decibels\n",
    "    \"\"\"\n",
    "    hop_length = len(samples)//256\n",
    "    \n",
    "    # get mel spectrogram image data\n",
    "    mel_features = librosa.feature.melspectrogram(\n",
    "        y=samples, sr=sr, hop_length=hop_length, n_mels=256)\n",
    "    \n",
    "    # clip the array to fit our target image shape\n",
    "    mel_features = mel_features[:, :256]\n",
    "    \n",
    "    # convert to decibels as and normalize the image for efficiency\n",
    "    mel_in_db = librosa.power_to_db(mel_features, ref=np.max)\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    return scaler.fit_transform(mel_in_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_augmentation(samples: np.array):\n",
    "\n",
    "    gaussian_noise = AddGaussianNoise(\n",
    "        min_amplitude=0.001,\n",
    "        max_amplitude= 0.015,\n",
    "        p=0.5\n",
    "    )\n",
    "    time_shift = Shift(\n",
    "        min_fraction=-0.2,\n",
    "        max_fraction=0.2,\n",
    "        rollover=False,\n",
    "        fade=True,\n",
    "        p=0.5\n",
    "    )\n",
    "    pitch_shift = PitchShift(\n",
    "        min_semitones=-0.5,\n",
    "        max_semitones=0.5,\n",
    "        p=0.25\n",
    "    )\n",
    "    gain = Gain(p=0.5)\n",
    "    augmenter = Compose(\n",
    "        [time_shift, gain, pitch_shift, gaussian_noise])\n",
    "    return augmenter(samples=samples,\n",
    "                     sample_rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'AVP-LVT_Dataset\\AVP_Dataset\\Personal'\n",
    "dataset = pd.DataFrame(columns = ['Specs','Label', 'Audio'])\n",
    "\n",
    "for subdir,_,files in os.walk(dataset_path):\n",
    "    for filename in (files): \n",
    "        \n",
    "        if filename.endswith('.csv'):\n",
    "            onset_csv = pd.read_csv(os.path.join(subdir,filename), names=['Onsets','Inst','Phenome','Sil'])\n",
    "            labels = onset_csv['Inst']\n",
    "            onsets = np.floor(np.array(onset_csv['Onsets']) * 44100)\n",
    "            onsets = onsets.astype(int)\n",
    "        if filename.endswith('.wav'):\n",
    "            x, sr = librosa.load(os.path.join(subdir,filename), sr = None)\n",
    "            for i in range(1,len(onsets)):\n",
    "                if i == 1:\n",
    "                    x_audio = x[(onsets[i-1]):onsets[i]]\n",
    "                else:\n",
    "                    x_audio = x[(onsets[i-1]-1000):onsets[i]]\n",
    "                #  x_temp = apply_augmentation(x_audio)\n",
    "                x_audio = librosa.util.fix_length(x_audio, 11025)\n",
    "                mel1 = get_mel_spectrogram(x_audio)\n",
    "                dataset =  dataset.append({'Specs':mel1, 'Label':labels[i-1], 'Audio':x_audio}, ignore_index=True)\n",
    "                x_temp = apply_augmentation(x_audio)\n",
    "                x_audio = librosa.util.fix_length(x_temp, 11025)\n",
    "                mel1 = get_mel_spectrogram(x_audio)\n",
    "                dataset =  dataset.append({'Specs':mel1, 'Label':labels[i-1], 'Audio':x_audio}, ignore_index=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.Label = pd.factorize(dataset.Label)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spectrogram_images(dataset):\n",
    "    #make directory\n",
    "    i = 0\n",
    "    for _, row in dataset.iterrows():\n",
    "        \n",
    "        specs = row[0]\n",
    "        label = row[1]\n",
    "\n",
    "        \n",
    "        directory = f'./spectrograms/{label}/'\n",
    "        if(os.path.isdir(directory)):\n",
    "            pass\n",
    "        else:\n",
    "            os.makedirs(directory, mode=0o777, exist_ok=True)\n",
    "        \n",
    "        plt.imsave(f'./spectrograms/{label}/spec_img{i}.png',specs, cmap='viridis')\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_spectrogram_images(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class category and index of the images: {'hhc': 0, 'hho': 1, 'kd': 2, 'sd': 3}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = datasets.ImageFolder(root='./spectrograms', transform=transforms.ToTensor())\n",
    "class_map=data.class_to_idx\n",
    "\n",
    "print(\"\\nClass category and index of the images: {}\\n\".format(class_map))\n",
    "length = len(data)\n",
    "\n",
    "\n",
    "train_data, val_data, test_data = torch.utils.data.random_split(data, [6693 , 2231, 2231])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 2016, 1: 1362, 0: 1534, 3: 1781})"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# labels in training set\n",
    "train_classes = [label for _, label in train_data]\n",
    "Counter(train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=8,num_workers=2, shuffle = True)\n",
    "val_loader = DataLoader(val_data, batch_size=8, num_workers=2, shuffle = True)\n",
    "test_loader = DataLoader(test_data, batch_size=1, num_workers=2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "   def __init__(self):\n",
    "      super().__init__()\n",
    "\n",
    "      self.layers = nn.Sequential(\n",
    "         nn.Conv2d(3, 32, kernel_size=5),\n",
    "         nn.MaxPool2d(2),\n",
    "         nn.ReLU(),\n",
    "         nn.Conv2d(32, 64, kernel_size=5),\n",
    "         nn.Dropout2d(),\n",
    "         nn.MaxPool2d(2),\n",
    "         nn.ReLU(),\n",
    "         nn.Flatten(),\n",
    "         nn.Linear(64 * 61 * 61, 200),\n",
    "         nn.ReLU(),\n",
    "         nn.Dropout2d(),\n",
    "         nn.Linear(200,4)\n",
    "\n",
    "\n",
    "      )\n",
    "      \n",
    "\n",
    "\n",
    "   def forward(self, x):\n",
    "      \n",
    "      x = self.layers(x)\n",
    "      \n",
    "\n",
    "      \n",
    "      return x\n",
    "   def predict(self, x):\n",
    "      x = self.layers(x)\n",
    "      x = torch.softmax(x)\n",
    "      pass\n",
    "model = Network().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Network                                  [8, 4]                    --\n",
       "├─Sequential: 1-1                        [8, 4]                    --\n",
       "│    └─Conv2d: 2-1                       [8, 32, 252, 252]         2,432\n",
       "│    └─MaxPool2d: 2-2                    [8, 32, 126, 126]         --\n",
       "│    └─ReLU: 2-3                         [8, 32, 126, 126]         --\n",
       "│    └─Conv2d: 2-4                       [8, 64, 122, 122]         51,264\n",
       "│    └─Dropout2d: 2-5                    [8, 64, 122, 122]         --\n",
       "│    └─MaxPool2d: 2-6                    [8, 64, 61, 61]           --\n",
       "│    └─ReLU: 2-7                         [8, 64, 61, 61]           --\n",
       "│    └─Flatten: 2-8                      [8, 238144]               --\n",
       "│    └─Linear: 2-9                       [8, 200]                  47,629,000\n",
       "│    └─ReLU: 2-10                        [8, 200]                  --\n",
       "│    └─Dropout2d: 2-11                   [8, 200]                  --\n",
       "│    └─Linear: 2-12                      [8, 4]                    804\n",
       "==========================================================================================\n",
       "Total params: 47,683,500\n",
       "Trainable params: 47,683,500\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 7.72\n",
       "==========================================================================================\n",
       "Input size (MB): 6.29\n",
       "Forward/backward pass size (MB): 191.03\n",
       "Params size (MB): 190.73\n",
       "Estimated Total Size (MB): 388.06\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, input_size=(8,3,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, cost, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, data in enumerate(dataloader):\n",
    "        \n",
    "        X, Y = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = cost(pred, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f'loss: {loss:>7f}  [{current:>5d}/{size:>5d}]')\n",
    "    return loss/batch\n",
    "\n",
    "def test(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, Y) in enumerate(dataloader):\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            pred = model(X)\n",
    "\n",
    "            test_loss += loss_function(pred, Y).item()\n",
    "            correct += (pred.argmax(1)==Y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "\n",
    "    print(f'\\nTest Error:\\nacc: {(100*correct):>0.1f}%, avg loss: {test_loss:>8f}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.032866  [    0/ 6693]\n",
      "loss: 0.050359  [  800/ 6693]\n",
      "loss: 0.081646  [ 1600/ 6693]\n",
      "loss: 0.042180  [ 2400/ 6693]\n",
      "loss: 0.219568  [ 3200/ 6693]\n",
      "loss: 0.080740  [ 4000/ 6693]\n",
      "loss: 0.277912  [ 4800/ 6693]\n",
      "loss: 0.233444  [ 5600/ 6693]\n",
      "loss: 0.050842  [ 6400/ 6693]\n",
      "\n",
      "Test Error:\n",
      "acc: 90.3%, avg loss: 0.035864\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.248597  [    0/ 6693]\n",
      "loss: 0.930870  [  800/ 6693]\n",
      "loss: 0.053731  [ 1600/ 6693]\n",
      "loss: 0.343077  [ 2400/ 6693]\n",
      "loss: 0.070092  [ 3200/ 6693]\n",
      "loss: 0.071630  [ 4000/ 6693]\n",
      "loss: 0.202826  [ 4800/ 6693]\n",
      "loss: 0.101344  [ 5600/ 6693]\n",
      "loss: 0.026999  [ 6400/ 6693]\n",
      "\n",
      "Test Error:\n",
      "acc: 90.6%, avg loss: 0.036604\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.002228  [    0/ 6693]\n",
      "loss: 0.290306  [  800/ 6693]\n",
      "loss: 0.016780  [ 1600/ 6693]\n",
      "loss: 0.182208  [ 2400/ 6693]\n",
      "loss: 0.153570  [ 3200/ 6693]\n",
      "loss: 0.009024  [ 4000/ 6693]\n",
      "loss: 0.415758  [ 4800/ 6693]\n",
      "loss: 0.000050  [ 5600/ 6693]\n",
      "loss: 0.063328  [ 6400/ 6693]\n",
      "\n",
      "Test Error:\n",
      "acc: 90.6%, avg loss: 0.036706\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.142653  [    0/ 6693]\n",
      "loss: 0.085502  [  800/ 6693]\n",
      "loss: 0.060603  [ 1600/ 6693]\n",
      "loss: 0.099709  [ 2400/ 6693]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mj:\\School\\ECE408Final\\Dataset-Separation.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/j%3A/School/ECE408Final/Dataset-Separation.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/j%3A/School/ECE408Final/Dataset-Separation.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/j%3A/School/ECE408Final/Dataset-Separation.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     train(train_loader, model, loss_function, optimizer)\n\u001b[0;32m      <a href='vscode-notebook-cell:/j%3A/School/ECE408Final/Dataset-Separation.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     test(val_loader, model)\n\u001b[0;32m      <a href='vscode-notebook-cell:/j%3A/School/ECE408Final/Dataset-Separation.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mDone!\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mj:\\School\\ECE408Final\\Dataset-Separation.ipynb Cell 20\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, cost, optimizer)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/j%3A/School/ECE408Final/Dataset-Separation.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(dataloader\u001b[39m.\u001b[39mdataset)\n\u001b[0;32m      <a href='vscode-notebook-cell:/j%3A/School/ECE408Final/Dataset-Separation.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/j%3A/School/ECE408Final/Dataset-Separation.ipynb#X25sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     X, Y \u001b[39m=\u001b[39m data[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mto(device), data[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/j%3A/School/ECE408Final/Dataset-Separation.ipynb#X25sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m      <a href='vscode-notebook-cell:/j%3A/School/ECE408Final/Dataset-Separation.ipynb#X25sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     pred \u001b[39m=\u001b[39m model(X)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for t in range(5):\n",
    "    print(f'Epoch {t+1}\\n-------------------------------')\n",
    "    train(train_loader, model, loss_function, optimizer)\n",
    "    test(val_loader, model)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = f'./Model'\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mj:\\School\\ECE408Final\\Dataset-Separation.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/j%3A/School/ECE408Final/Dataset-Separation.ipynb#X50sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test(test_loader,model)\n",
      "\u001b[1;32mj:\\School\\ECE408Final\\Dataset-Separation.ipynb Cell 22\u001b[0m in \u001b[0;36mtest\u001b[1;34m(dataloader, model)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/j%3A/School/ECE408Final/Dataset-Separation.ipynb#X50sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m test_loss, correct \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/j%3A/School/ECE408Final/Dataset-Separation.ipynb#X50sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> <a href='vscode-notebook-cell:/j%3A/School/ECE408Final/Dataset-Separation.ipynb#X50sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch, (X, Y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[0;32m     <a href='vscode-notebook-cell:/j%3A/School/ECE408Final/Dataset-Separation.ipynb#X50sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         X, Y \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device), Y\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/j%3A/School/ECE408Final/Dataset-Separation.ipynb#X50sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         pred \u001b[39m=\u001b[39m model(X)\n",
      "File \u001b[1;32mj:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mj:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1356\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[0;32m   1358\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1359\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[0;32m   1360\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1361\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[0;32m   1362\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mj:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1325\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1322\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1323\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1324\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m-> 1325\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[0;32m   1326\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[0;32m   1327\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mj:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1163\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1150\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m   1151\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1152\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1160\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1161\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1162\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1163\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m   1164\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[0;32m   1165\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1166\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1167\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1168\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32mj:\\Anaconda\\lib\\multiprocessing\\queues.py:117\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll():\n\u001b[0;32m    116\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m--> 117\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_recv_bytes()\n\u001b[0;32m    118\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sem\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    119\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mj:\\Anaconda\\lib\\multiprocessing\\connection.py:221\u001b[0m, in \u001b[0;36m_ConnectionBase.recv_bytes\u001b[1;34m(self, maxlength)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[39mif\u001b[39;00m maxlength \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m maxlength \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    220\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mnegative maxlength\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 221\u001b[0m buf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_recv_bytes(maxlength)\n\u001b[0;32m    222\u001b[0m \u001b[39mif\u001b[39;00m buf \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bad_message_length()\n",
      "File \u001b[1;32mj:\\Anaconda\\lib\\multiprocessing\\connection.py:306\u001b[0m, in \u001b[0;36mPipeConnection._recv_bytes\u001b[1;34m(self, maxsize)\u001b[0m\n\u001b[0;32m    304\u001b[0m bsize \u001b[39m=\u001b[39m \u001b[39m128\u001b[39m \u001b[39mif\u001b[39;00m maxsize \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mmin\u001b[39m(maxsize, \u001b[39m128\u001b[39m)\n\u001b[0;32m    305\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 306\u001b[0m     ov, err \u001b[39m=\u001b[39m _winapi\u001b[39m.\u001b[39;49mReadFile(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle, bsize,\n\u001b[0;32m    307\u001b[0m                                 overlapped\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    308\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m         \u001b[39mif\u001b[39;00m err \u001b[39m==\u001b[39m _winapi\u001b[39m.\u001b[39mERROR_IO_PENDING:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test(test_loader,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Error:\n",
      "acc: 90.9%, avg loss: 0.036239\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(val_loader,model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "03693f3c6a313b5d3a7c2b7ec8ef7be2d4e0715aee21eda25f623623dca7cf10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
